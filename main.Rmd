---
title: 'Práctica 2: Limpieza y validación de los datos'
author: "Luis Manuel Pérez Geraldino y Sergi Ramirez Mitjans"
date: "16 de Mayo de 2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
header-includes:
- \usepackage[spanish]{babel}
- \usepackage{longtable}
- \usepackage{color}
- \usepackage{float}
- \usepackage{ltxtable}
- \usepackage{tabularx}
- \usepackage{fancyhdr}
- \usepackage[font=small,labelfont=bf]{caption}
---

```{r instalación paquetes, eval = FALSE, echo = FALSE}
# Instalar paquetes de R que necesitaremos
RequiredPackages <- c("Hmisc", "dplyr", 'VIM', 'FactoMineR', 'tidyr', 'dplyr', 
                      'magrittr', 'plspm', 'amap')
for (i in RequiredPackages) { #Installs packages if not yet installed
    if (!require(i)) install.packages(i)
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    fig.width   = 10,       # the width for plots created by code chunk
    fig.height  = 5,       # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
  # fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = FALSE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE,     # if FALSE knitr will not display any warning messages in the final document
    comment     = FALSE) 
options(warn=-1)
options(scipen=999)
options(tinytex.verbose = TRUE)
```

# 1. Descripción del dataset

El dataset que a continuación se va a describir, se a obtenido mediante *web scrapping* en la práctica 1 de esta misma asignatura por lo tanto está práctica se tratará de realizar la limpieza del dataset que se consiguió desarrollar en etapas anteriores de esta asignatura. 

El conjunto de datos obtenido recoge los medicamentos con más facturación en el mundo farmacéutico. A la vez también esta incorporado los medicamentos más consumidos en España. Este dataset es variable ya que se tendrán tantos medicamentos como el código demande. Esto es posible ya que el código esta preparado para hacer de buscador y extractor de aquellos medicamentos que se pasen por parámetros. Algunas variables de las que encontramos en el dataset sería el precio, nombre del medicamente, genérico o url del Vademecum.

```{r Lectura de datos}
# Leemos los datos para guardar
datos <- read.csv2('data/data.csv',  sep = ";", header = T, stringsAsFactors = T, dec = ".", encoding = 'latin1')

# Cambiamos el nombre de las columnas para que puedan ser leibles
cols <- c("Medicamento", "URL", "Presentacion", "Codigo_Nacional", "Tipo", "Generico", "Laboratorio", "Estado", "Fecha_Alta", "Fecha_Baja", "Aportacion_Beneficiario", "Principio_Activo", "PVP", "Precio_Referencia", "Menor_Precio_Agrupacion_Homogenea", "Agrupacion_Homogenea", "Diagnostico_Hospitalario", "Tratamiento_Larga_Duracion", "Control_Medico", "Huerfano", "Enfermedad")
colnames(datos) <- cols
```

A continuación se van a describir el significado de cada variable: 

```{r Descripción de variables}
library(readxl)
library(xtable)
archivos <- 'data/codbook_variables.xlsx'
codbook_vademecum <- data.frame(read_excel(archivos, sheet = 1))
codbook <- data.frame(read_excel(archivos, sheet = 2))
codbook <- rbind(codbook_vademecum, codbook)
codbook <- xtable(codbook, include.rownames=FALSE)
align(codbook) <- "|l|l|l|X|"
print(codbook, tabular.environment="tabularx", width="\\textwidth", comment = FALSE)
```

## 2. Objetivos

La creación del estudio de nuestra base de datos contiene diferentes objetivos. A continuación vamos a explicar cada uno de ellos para que tenga sentido el echo de realizar según que prueba estadística o modelo de predicción. Los objetivos son los siguientes:
  
  - Para empezar queremos realizar un estudio para saber si existe o no existe diferencia entre el precio y los grupos tipo de medicamento.
  
  - También queremos hacer un estudio para encontrar existencia diferencias en el precio según el laboratorio que realiza el medicamento. 
  
  - Mirar si existe diferencias de precio según la enfermedad que se quiera tratar. 
  
  - Agrupar conjunto de medicamentos para detectar patrones subjacentes detrás de ello. 
  
  - Intentar hacer una predicción del precio. Este apartado consistirá en realizar un modelo de predicción de precio. Nos servirá a las farmacéuticas para facilitarles el precio que se le debe de asignar a un enfermo según variables del medicamento que va a utilizar. 
  
  - Realizar una clasificación de la enfermedad según variables económica y del laboratorio que lo fabrica. Al final queremos hacer este estudio para poder ayudar a las empresas aseguradoras para renovar o no un seguro de salud sabiendo el gasto en medicamentos que realiza el cliente saber que enfermedad tiene. 
  
# 3. Análisis Descriptivo

A continuación vamos a hacer un descriptivo numérico de como son nuestras variables. 

```{r Clasificación de variables}
# Generamos 3 vectores con la posicion de las columnas según la tipologia de ellas
lista_clase <- sapply(datos, class)

# Generamos diferentes variables
var_factor    <- unname(which(lista_clase %in% c('factor', 'character')))
var_numeric   <- unname(which(lista_clase %in% c('numeric', 'integer')))
```


## 3.1 Variables numéricas

Primoro de todo vamos a realizar un estudio de las variables numéricas que tenemos en nuestro dataset. En él sólo encontramos las variables `PVP`, `Precio_Referencia` y `Menor_Precio_Agrupacion_Homogenea`.

```{r Descriptivo variables numéricas}
cols <- colnames(datos)[var_numeric]
cols <- cols[which(!cols %in% c('Codigo_Nacional'))]
library(pastecs)
tabla <- round(stat.desc(datos[, cols]), 2)
print(tabla)
tabla <- xtable(tabla, include.rownames=FALSE)
print(tabla, tabular.environment="longtable", comment = FALSE)

par(mfrow = c(1, 3))
for(col in cols){
    cat("Descriptivo univariante para la variable ", col)
  	grafico <- boxplot(datos[, col], col = "steelblue2", main = col)
}
par(mfrow = c(1, 1))
```

Como podemos ver en la tabla de descriptivos, vemos que las variables `Precio_Referencia` y `Menor_Precio_Agrupacion_Homogenea` contienes valores faltantes. Esto nos hará replantearnos la necesidad de imputar o no los valores faltantes de estas variables. 

Por lo que respecta a la distribución de los datos numéricos según el boxplot, podemos ver como el precio que viene de la variable `PVP` tiene una distribución más escueta de sus rangos quartílicos pero esto es derivado a que la variable `PVP` contiene valores mucho más altos que en las otras dos variables no las tiene y por lo tanto hace que se detecten outliers y la consecuencia de la forma 'achatada' de la distribución de la variable. 

## 3.2 Varibles categóricas

A continuación vamos a generar la descriptiva de las variables categóricas. En este apartado realizaremos tablas de frequencia para ver como está distribuido el número de casos para cada categoria. A demás se va a representar mediante barplots (ya que si lo hicieramos con diagrama de sectores no es tan visual ver la proporción correspondiente a cada categoria).

Debido a que tenemos algunas variables con muchas categorias, restringiremos la creación de los gráficos a sólo aquellos que tengan máximo 10 categorias. Esto se debe a que si tenemos muchas categorias el gráfico no es entendible en ningun caso. 

También para evitar que no sea leible cada categoria, haremos que el gráfico de barplot sea en horizontal. 

Antes de eso, vamos a cambiar el nombre de algunas de las categorias de algunas de las variables para que cuando se representen sean de fácil lectura. 

```{r Recategorización de variables}
library(plyr)

# Tipo
datos[, 'Tipo'] <- revalue(datos[, 'Tipo'], c("Medicamento Etica" = "Etica",  
                           "Medicamento Generico" = "Generico",
                           "Medicamento con Denominacion Generica" = "Denominacion Generica"))
# Estado
datos[, 'Estado'] <- revalue(datos[, 'Estado'], c("BAJA POR NO COMERCIALIZAR" = "BAJA NO COMERCIALIZAR",  
                           "SUSPENSION TEMPORAL GENERAL" = "SUSPENSION TEMP GRAL"))
# Enfermedad
datos[, 'Enfermedad'] <- revalue(datos[, 'Enfermedad'], 
                         c("COAGULOS SANGUINIOS EN REPOSO" = "ANTICOAGULANTE",  
                           "ENFERMEDADES CARDIOVASCULARES" = "ENF. CARDIOVASCULARES",
                           "ENFERMEDADES DE LA ARTERIA CORONARIA" = "ENF. CARDIOVASCULARES",
                           "ENFERMEDADES RESPIRATORIAS" = "ENF. RESPIRATORIAS"))
```

```{r Descriptivo variables categóricas}
cols <- colnames(datos)[var_factor]

for(col in cols) {
  cat("Descriptivo univariante para la variable", col, "\n")
  tabla_frec <- table(datos[, col], useNA = "ifany")
  longitud <- length(tabla_frec)
  if (longitud <= 16) {
		counts <- data.frame(tabla_frec)
		colnames(counts) <- c('Variables', 'Conteo')
		counts[, 'Porcentaje'] <- round((counts$Conteo/sum(counts$Conteo))*100, 2)
    par(mai=c(1,2,2,2))
    grafico <- barplot((counts$Porcentaje/100) , main = paste0("Barplot de la variable ", col), horiz = TRUE, names.arg = counts$Variables,las = 1, col = "darkblue", xlim = c(0, 1), cex.names = 0.6)
    labels = paste0(counts$Conteo, " (", counts$Porcentaje,"%)")
    text(x = (counts$Porcentaje/100), col = "red", y = grafico, label = labels, pos = 4, cex = 0.65)
  } else {
    cat("El número de categorias de la variable ", col, "es muy grande(", longitud, "). Pasamos a la siguiente variable.\n")
		cat("\n ===============================\n")
  }
}
```

Como podemos observar con el código que se ha ejecutado anteriormente, vemos como tenemos dos problemas en las variables. 

1) Las variables `Medicamento` (908), `URL` (908), `Presentacion` (132), `Laboratorio` (73), `Fecha_Alta` (227), `Fecha_Baja` (45), `Principio_Activo` (53), `Agrupacion_Homogenea` (138) son variables con muchas categorias y por lo tanto nos deberemos de plantear si las usamos o no para generar nuestros modelos. En caso de que las quisieramos usar, deberiamos de recategorizar variables para tener un número reducido de ellas. 

2) Las variables ``Diagnostico_Hospitalario`, `Control_Medico` y `Huerfano` son variables que sólo tienen 1 categoria. Por lo tanto decidimos no usarlas en nuestro estudio y posteriormente eliminarlas del dataframe. 

Viendo los barplots podemos intentar pensar en juntar categorias de variables. En nuestro caso, para la variable `Enfermedad` deberiamos de juntar dichas categorias. Cómo realmente necesitamos decisiones expertas de parte de medicos para poder juntar diagnósticos semejantes, seguiremos adelante el trabajo sin agrupar categorias. 

# 4. Análisis Descriptivo Bivariante

A continuación vamos hacer un estudio de como se comportan las variables de nuestro dataset teniendo en cuenta la variable precio (`PVP`). Hemos decidido la variable precio ya que es la variable numérica que utilizaremos para responder a nuestras preguntas e validar nuestros contrastes. 

### 4.1 Variables numéricas

A continuación vamos a realizar una matriz de correlaciones para poder detectar variables muy correlacionadas e incluso variables que sean iguales. Este paso también nos servirá para descartar variables que sean muy similares y por lo tanto su correlación sea igual a 1. Este es un método de reducir dimensionalidad. Otra forma sería realizando un análisis de componentes principales (ACP) peró descartamos este método ya que sólo disponemos de 3 variables numéricas. 

Para poder llevar a cabo nuestras correlaciones, y ya que hacemos el estudio antes de imputar valores, debemos decir a la instrucción de R `cor` que sólo tenga en cuenta los casos que si se tiene información de ello.

```{r Descriptivo bivariante variables numéricas}
library(xtable)
cols <- c("PVP", "Precio_Referencia", "Menor_Precio_Agrupacion_Homogenea")
correlaciones <- cor(datos[, cols], use = "complete.obs")
tabla <- xtable(correlaciones, include.rownames = FALSE)
print(tabla, tabular.environment="longtable", width="\\textwidth", comment = FALSE)
```

Por lo tanto del output podemos extraer la información de que las tres variables numéricas tienen una correlación perfecta (1) y por lo tanto podemos eliminar de nuestros datos 2 de ellas. 

Eliminaremos las variables `Precio_Referencia` y `Menor_Precio_Agrupacion_Homogenea` ya que no tiene sentido que posteriormente se haya de imputar estos valores para que nos quede igual que la variable `PVP`. Por lo tanto no hará tampoco falta imputación de datos faltantes en las variables numéricas. 


### 4.1 Variables categóricas

Para realizar dichas pruebas, haremos el contraste de Kruskal-Wallis para detectar independencia entre grupos y las graficaremos con boxplots múltiples. Para ello cogeremos las mismas variables que habiamos cogido anteriormente para realizar los gráficos univariantes. 

```{r Descriptivo bivariante variables categóricas}
cols <- c("Tipo", "Estado", "Aportacion_Beneficiario", "Tratamiento_Larga_Duracion", "Enfermedad")
for(col in cols) {
  boxplot(formula(paste0("PVP ~", col)) , datos, col = rainbow(length(unique(datos[, col]))), main = formula(paste0("PVP ~", col)), horizontal=TRUE, las = 1, cex.names = 0.34, names.arg = unique(datos[, col]))
}

```

Podemos sacar diferentes conclusiones que nos ayudarán a entender posibles respuestas en le futuro. 

Si miramos el gráfico de Enfermedades vemos como los medicamentos anticuagulantes són aquellos que tienes más variabilidad de precios. Esto nos hace pensar que igual dichos medicamentos que intentan evitar esa enfermedad deberian de estudiarse por separado ya que pueden hacer (y lo hacen) obtener outliers en nuestros datos y crear un sesgo por el simple hecho de ser un medicamento anticuagulante. Estos medicamentos, són aquellos medicamentos que necesitan gente especialmente destinada a enfermedades crónicas y por eso en el boxplot de Aportación vemos qu se encontrarian en la categoria ESPECIAL. Además sabemos que los aticuagulantes sirven para espesar la sangre y por lo tanto no se necesita un tratamiento de larga duración para su uso, así lo vemos reflejado en nuestro boxplot. 

# 5. Limpieza de los datos

A continuación generamos la etapa de limpieza de datos. En ella se intentará preparar el dataset para poder usarlo para hacer contrastes y predicciones de valores en u futuro. Además en esta etapa se pretende también crear nuevas variables, contrastar que los datos numéricos siguen una distribución normal e detectar posibles outliers de nuestros datos. 

## 5.1 Creación de nuevas variables

Primero de todo vamos a discretizar la variable `PVP`. 

Aunque sabemos que la discretización de una variable numérica hace que dejes de tener información de la distribución de la variable numérica, para representaciones posteriores necesitamos tener la variable `PVP` discretizada para poder representarla por ello vamos a crear la a continuación. 

Cuando hablamos de discretizar una variable, se define como el remplazar variables numéricas a atributos categóricos con etiquetas. 

Para realizar la discretización vamos a realizar métodos de clustering. Este tipo de modelo k-means consiste en buscar aquellas clases que sean homogeneas en ellas mismas pero entre ellas sean suficientemente separables.Por lo tanto dividen sus valores en grupos teniendo en cuenta la distribución del atributo y la  cercanía de los datos a cada grupo.

```{r Discretización variable precio}
library(arules)
# k-means clustering
discretizacion <- discretize(datos[, 'PVP'], method = "cluster", breaks = 2, onlycuts = TRUE)
datos[, 'tram_PVP'] <- cut(datos[, 'PVP'], breaks = discretizacion, include.lowest = TRUE)
```

Seguidamente creamos una variable llamada `tram_presentacion` donde lo que se hará será agrupar categorias de presentación de tal forma tener menos niveles en la variable. Mediante el text mining de la variable `Presentación`.

```{r Creación variable presentación}
# ENVASE
datos[, 'tram_presentacion'] <- 'ENVASES'

# INHALADORES/OTROS
patron <- 'inhalador|inhalación|inh|tiras'
qui <- grep(patron, tolower(datos[, 'Presentacion']))
datos[qui, 'tram_presentacion'] <- 'INHALADORES/OTROS'

# INJECTABLES
patron <- 'opa|pluma|plumas|cartucho|cartuchos|jeringa|jeringas|injectables'
qui <- grep(patron, tolower(datos[, 'Presentacion']))
datos[qui, 'tram_presentacion'] <- 'INJECTABLES'

# SOBRES
patron <- 'vial|sobre|sobres'
qui <- grep(patron, tolower(datos[, 'Presentacion']))
datos[qui, 'tram_presentacion'] <- 'SOBRES'

# FRASCO
patron <- 'frasco|frascos|bote|botes|ampolla|ampollas'
qui <- grep(patron, tolower(datos[, 'Presentacion']))
datos[qui, 'tram_presentacion'] <- 'FRASCOS'

# BLÍSTER
patron <- 'blister|blíster|comp|comprimidos|cap|cápsula|cápsulas'
qui <- grep(patron, tolower(datos[, 'Presentacion']))
datos[qui, 'tram_presentacion'] <- 'BLISTER'
```

Seguidamente, generamos la variable `Baja`. Esta variable es una variable dicotómica extraida de la variable `Fecha_Baja` que nos permitirá saber si el medicamento se ha dado de baja en y por lo tanto no se encuentra ni a la venta ni financiado por el sistema nacional de salud. 

```{r Creación variable baja}
# Creamos la variable Baja que consiste en si el medicamento esta o no esta dado de baja
datos[, 'Baja'] <- ifelse(datos[, 'Fecha_Baja'] == "", "NO", "SI")
```

El siguiente paso es generar dos nuevas variables. Estas nuevas variables serán la codificación de las variables `Agrupacion_Homogenea` y `Laboratorio`. Hemos tomado esta decisión ya que queremos evitar que por motivos de escritura de las categorias de la variable encontremos varios laboratorios que se refieren al mismo escrito de forma diferente o varias agrupaciones homogeneas que le sucedan lo mismo. 

```{r Creación variable codigo agrupacion}
# Separamos el código de la agrupación
codigos <- strsplit(as.character(datos[, 'Agrupacion_Homogenea']), "-")
data = data.frame(codigo = character(), agrupacion = character(), stringsAsFactors=FALSE)

for (i in 1:length(codigos)){
    if(codigos[i][1] == ""){
      data[i, 1] = NA
      data[i, 2] = NA
    } else {
      data[i, 1] = as.character(codigos[[i]][1])
      data[i, 2] = as.character(codigos[[i]][2])
    }
}

datos[, 'Codigo_Agrupacion'] <- data[, 'codigo']
# Aseguraremos el mismo número de dígitos en el código
datos[, 'Codigo_Agrupacion'] <- sprintf("%0.5d", as.numeric(datos[, 'Codigo_Agrupacion']))
```

Seguimos con la variable `Laboratorio`.

```{r Creación variable codigo laboratorio}
# Realizamos la separación por laboratorio
lab <- strsplit(as.character(datos[, 'Laboratorio']), ",")
data = data.frame(codigo = character(), laboratorio = character(), sociedad =  character(), stringsAsFactors = FALSE)

for (i in 1:length(codigos)){
    if(length(lab[[i]]) == 3){
      valor1 = as.character(lab[[i]][1])
      valor2 = as.character(lab[[i]][2])
      valor3 = as.character(lab[[i]][3])
    } else {
      if (length(lab[[i]]) == 2){
      valor1 = as.character(lab[[i]][1])
      valor2 = as.character(lab[[i]][2])
      valor3 = ""
      } else {
      valor1 = ""
      valor2 = as.character(lab[[i]][2])
      valor3 = ""
      }
    }
    data[i, 1] = valor1
    data[i, 2] = valor2
    data[i, 3] = valor3
}

datos[, 'Codigo_Laboratorio'] <- data[, 'codigo']

# Aseguraremos el mismo número de dígitos en el código
datos[, 'Codigo_Laboratorio'] <- sprintf("%0.4d", as.numeric(datos[, 'Codigo_Laboratorio']))
```

Antes de eliminar la variable `Laboratorio`, vamos a aprovechar dicha variable para crear una nueva variable que la llamaremos `Sociedad`. Esta nueva variable consiste en identificar cada medicamento de su laboratorio si el laboratorio es de una Sociedad anónima (S.A.), Sociedad anónima unipersonal (S.A.U.), sociedad limitada (S.L.) y sociedad limitada unipersonal (S.L.U.).

```{r Creación variable sociedad}
datos[, 'Sociedad'] <- 'Sin sociedad'

# Sociedad anónima (S.A.)
patron <- 'S.A.|S.A'
qui <- grep(patron, toupper(datos[, 'Laboratorio']))
datos[qui, 'Sociedad'] <- 'S.A.'

# Sociedad anónima unipersonal (S.A.U.)
patron <- 'S.A.U.|S.A.U'
qui <- grep(patron, toupper(datos[, 'Laboratorio']))
datos[qui, 'Sociedad'] <- 'S.A.U.'

# Sociedad limitada (S.L.)
patron <- 'S.L.|S.L'
qui <- grep(patron, toupper(datos[, 'Laboratorio']))
datos[qui, 'Sociedad'] <- 'S.L.'

# Sociedad limitada unipersonal (S.L.U.)
patron <- 'S.L.U.|S.L.U'
qui <- grep(patron, toupper(datos[, 'Laboratorio']))
datos[qui, 'Sociedad'] <- 'S.L.U.'
```

Antes de acabar con el apartado de la creación de las nuevas variables, vamos a generar los gráficos y tablas de frequencias de las nuevas variables para hacernos una idea de como están distribuidas y como hemos mejorado. 

```{r Descriptivo univariante nuevas variables}
cols <- c("tram_PVP", "tram_presentacion", "Baja", "Sociedad")

for(col in cols) {
  cat("Descriptivo univariante para la variable", col, "\n")
  tabla_frec <- table(datos[, col], useNA = "ifany")
  longitud <- length(tabla_frec)
  if (longitud <= 16) {
		counts <- data.frame(tabla_frec)
		colnames(counts) <- c('Variables', 'Conteo')
		counts[, 'Porcentaje'] <- round((counts$Conteo/sum(counts$Conteo))*100, 2)
		print(counts)
    cat("\n ===============================\n")
    par(mai=c(1,2,2,2))
    grafico <- barplot((counts$Porcentaje/100) , main = paste0("Barplot de la variable ", col), horiz = TRUE, names.arg = counts$Variables,las = 1, col = "darkblue", xlim = c(0, 1), cex.names = 0.6)
    labels = paste0(counts$Conteo, " (", counts$Porcentaje,"%)")
    text(x = (counts$Porcentaje/100), col = "red", y = grafico, label = labels, pos = 4, cex = 0.65)
  } else {
    cat("El número de categorias de la variable ", col, "es muy grande(", longitud, "). Pasamos a la siguiente variable.\n")
		cat("\n ===============================\n")
  }
}

```

Como comprobamos, tenemos unos grupos realmente distribuidos y con los que podemos trabajar para realizar predicciones. Por lo tanto según nuestros boxplots, estas nuevas variables son suficientemente representativas para poder hacer uso de ellas. 

## 5.2 Seleccion de las variables

A continuación se detallará que variables del dataset se descartará y cuales son los motivos que nos hacen descartar las variables. Estas variables son: 
  - `Medicamento`: Es el nombre del medicamento y por lo tanto no lo necesitamos para generar modelos ya que sólo lo necesitamos para identifcar cada registro.
  - `URL`: Es una variable que enlaza cada url del vademecum con el registro del medicamento.
  - `Precio_Referencia`: Descartamos esta variable ya que detectamos que tenian NA's y que era igual que la variable `PVP` al tener correlación de 1.
  - `Fecha_Alta`: Descartamos la variable ya que no trabajaremos con fechas de introducción de la información en la base de datos ya que no nos aporta información. 
  - `Fecha_Baja`: Descartamos la variable ya que no nos aporta información.
  - `Menor_Precio_Agrupacion_Homogenea`: Descartamos esta variable ya que detectamos que tenian NA's y que era igual que la variable `PVP` al tener correlación de 1.
  - `Generico`: Eliminamos esta variable ya que no tiene información (es toda NA's)
  - `Presentacion`: Escogemos la variable creada `tram_presentacion` en lugar de presentación para evitar tener una variable con muchas categorias. 
  - `Agrupacion_Homogenea`: Escogemos la variable creada `Codigo_Agrupacion` en su defecto para evitar que hayan posible problemas en la unificación de categorias escritas de diferentes formas. 
  - `Laboratorio`:  Escogemos la variable creada `Codigo_Laboratorio` en su defecto para evitar que hayan posible problemas en la unificación de categorias escritas de diferentes formas. 

Por lo tanto nos quedamos con las variables `Tipo`, `Estado`, `Aportacion_Beneficiario`, `Principio_Activo`, `PVP`, `Tratamiento_Larga_Duracion`, `Enfermedad` y `Codigo_Agrupacion`. También nos quedamos con las variables creadas `Tipo`, `Codigo_Laboratorio`, `tram_PVP`, `tram_presentacion` y `Sociedad`.

```{r Selección variables modelo}
cols <- c("Tipo", "Codigo_Laboratorio", "Estado", "Baja", "Aportacion_Beneficiario", "Principio_Activo", "PVP", "tram_PVP", "Tratamiento_Larga_Duracion", "Enfermedad", "tram_presentacion", "Sociedad", "Codigo_Agrupacion")

datos_sel <- datos[, cols]
```

## 5.3 Imputación de variables

En este apartado no deberemos imputar nada debido a que las variables numéricas seleccionadas, no se han de imputar, ya que son completas. Las variables descartadas eran las únicas que contenian NA's y que deberiamos de haber imputado pero no las hemos cogido para hacer los modelos ya que anteriormente hemos visto que las correlaciones entre `PVP` y las otras dos era perfecta ($\rho = 1$). Por lo tanto, si se hubiera necesitado imputar los valores lo hubieramos hecho a través de paquetes que tuvieran en cuenta la distribución de las variables informadas. 


## 5.4 Ceros y elementos vacíos
Tal y como se explica en el apartado anterior, al haber seleccionado algunas variables hemos hecho que tuvieramos menos casos a tener en cuenta como ceros y elementos vacíos. Por lo tanto a continuación vamos a generar una tabla para identificar los NA's correspondientes para cada variable y veremos como cambiaremos estos NA's. 

```{r}
NAs <- data.frame(absoluto = colSums(is.na(datos_sel)))
NAs[, 'porcentaje'] <- round((NAs$absoluto/nrow(datos_sel))*100, 2)
NAs
```

Vemos como obtenemos un 5% de valores perdidos en la variable `Agrupacion_Homogenea` y la variable que deriva de ella `Codigo_Agrupacion`. Por lo tanto deberemos de tratarla para que desaparezcan este porcentaje de NA's. 

Para ello crearemos una nueva categoria de la variable que sea `99999`. Este codigo lo que nos estará diciendo es que será una nueva variable que la llamaremos SIN AGRUPACIÓN. 

```{r}
qui <- which(is.na(datos_sel[, 'Codigo_Agrupacion']))
datos_sel[qui, 'Codigo_Agrupacion'] <- '99999'
```


Volvemos a examinar el número de NA's que contienen en nuestros datos y observaremos como ya no tenemos NA's en ellos. 

```{r}
NAs <- data.frame(absoluto = colSums(is.na(datos_sel)))
NAs[, 'porcentaje'] <- round((NAs$absoluto/nrow(datos_sel))*100, 2)
NAs
```


## 5.5 Normalidad de las variables 

A continuación vamos a analizar si nuestra variable `PVP` sigue una distribución normal o por el contrario no la sigue y deberemos de hacer tansformaciones. 

Primero de todo vamos a visualizar los gráficos para ver si realmente son normales. 

```{r}
library(ggplot2)

par(mfrow = c(1, 2))
ggplot(data = datos_sel, aes(x = PVP)) +
  geom_histogram(aes(y = ..density.., fill = ..count..)) +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "firebrick",
                args = list(mean = mean(datos_sel$PVP),
                            sd = sd(datos_sel$PVP))) +
  ggtitle("Histograma con la curva de densidad") +
  theme_bw()

qqnorm(datos_sel$PVP, pch = 19, col = "gray50")
qqline(datos_sel$PVP)
par(mfrow = c(1, 1))

```

Vemos como realmente nos está hablando de que nuestros datos no son normales ya que los puntos no se distributen encima de la linia y el histograma nos informa de que su forma distribuida podria ser como una $\chi^2$. 

Vamos a comprobar lo que nos dice los gráficos mediante dos contrastes estadísticos: 

```{r}
print(shapiro.test(x = datos_sel$PVP))
print(ks.test(x = datos_sel$PVP, "pnorm", mean(datos_sel$PVP), sd(datos_sel$PVP)))
library("nortest")
lillie.test(x = datos_sel$PVP)
library("tseries")
jarque.bera.test(x = datos_sel$PVP)
```

Como podemos comporbar ninguno de los cuatro tests de comprobación de normalidad nos indica que nuestros datos se distribuyan de forma normal. (Generamos 4 contrastes diferentes ya que cada uno de ellos se pueden considerar mas o menos restrictivos a la hora de calcular la normalidad). 

Por lo tanto los constrastes nos están indicando que no existe normalidad en los datos. 

A continuación realizaremos una transformación logarítmica a ver si corregimos el efecto del sesgo relacionado con el precio.


```{r}
datos_sel$log_PVP <- log(datos_sel$PVP)
library(ggplot2)

par(mfrow = c(1, 2))
ggplot(data = datos_sel, aes(x = log_PVP)) +
  geom_histogram(aes(y = ..density.., fill = ..count..)) +
  scale_fill_gradient(low = "#DCDCDC", high = "#7C7C7C") +
  stat_function(fun = dnorm, colour = "firebrick",
                args = list(mean = mean(datos_sel$log_PVP),
                            sd = sd(datos_sel$log_PVP))) +
  ggtitle("Histograma con la curva de densidad") +
  theme_bw()

qqnorm(datos_sel$log_PVP, pch = 19, col = "gray50")
qqline(datos_sel$log_PVP)
par(mfrow = c(1, 1))

```

A simple vista y tal y como nos muestra los gráficos podemos ver que los datos han sido normalizados excepto en una pequeña porporcion de ellos entorno al 1. Vamos a aplicar los contrastes a ver si nos verifican realmente que existe normalidad una vez realizado el logaritmo. 

```{r}
shapiro.test(x = datos_sel$log_PVP)
ks.test(x = datos_sel$log_PVP, "pnorm", mean(datos_sel$log_PVP), sd(datos_sel$log_PVP))
library("nortest")
lillie.test(x = datos_sel$log_PVP)
library("tseries")
jarque.bera.test(x = datos_sel$log_PVP)
```

Aunque visualmente veíamos una corrección en los datos a normales, hemos podido comprobar que tampoco conseguimos que sea normal. Nos replanteamos la idea de hacer la misma prueba para la raíz cuadrada y seguiamos considerando NO normalidad de los datos. Para acabar el proceso intentamos realizar los mismos pasos pero agrupando en baratos y caros y nos continuaba saliendo que no exisita normalidad en nuestros datos.

En definitiva, trabajaremos con la variable `PVP` aunque sepamos que no sigue una distribución normal. Esto pasa porque cuando se diseño el experimento de extracción de la información se fue a coger directamente aquellos medicamentos contra la cura de enfermedades importantes y además aquellos medicamentos con más ingresos por parte de los laboratorios con su distribución. Por lo tanto entendemos perfectamente que los datos al final no se distribuyan como una normal. 

Seguidamente comprobaremos como los constrastes nos hablaran realmente de que existe diferencias según el laboratorio, el tipo y otras características. Esto nos da mas motivos para pensar que el diseño de experimento fue erroneo y las consequencias han sido la de tener datos no normalizados.  

Una posible solución de normalización de los datos sería realizando la estandarización por el máximo pero en este trabajo no hemos aprofundizado por esta via. 


## 5.6 Detección de outliers

Para la detección de outliers vamos a trabajar con el contraste de Bonferonni y las distancias de Cook. 

Primero de todo graficamos las distancias de Cook. 

```{r}
modelo <- lm(PVP ~ ., data = datos_sel)
cooksd <- cooks.distance(modelo)
plot(cooksd, pch="*", cex=2, main="Observaciones influyentes según la distancia de Cook")  
abline(h = 4*mean(cooksd, na.rm = T), col = "red")  
text(x = 1:length(cooksd) + 1, y = cooksd, labels = ifelse(cooksd>4*mean(cooksd, na.rm = T),names(cooksd),""), col = "red") 
```

Comprobamos como todos aquellos valores por encima de la línia roja se considerarán que son outliers. Y por lo tanto tenemos bastantes casos considerados outliers. 

A continuación vamos a realiziar el test de Bonferonni para detectar los outliers que tensmo en nuestra base de datos. 

```{r}
library(car)
outlierTest(modelo)
```

Por lo tanto, el test de Bonferonni nos está hablando de que existen 10 registros que són considerados Outliers y que deberian eliminarse de la base de datos. 

Nosotros nos quedamos con esos valores ya que consideramos que han sido generados por un mal diseño experimental (ya que se han descargado medicamentos escogidos y no al azar). 

# 6. Contrastes

Seguidamente vamos a realizar contraste de hipótesis para poder resolver algunas de las cuestiones que se planteaban en el objetivo de este informe. 

## 6.1 Precio vs. Tipo de Medicamento

A continuación queremos ver si existen diferencias o no en el precio entre diferentes tipos de medicamentos. 

#### Hipótesis nula y alternativa

- $H_{0}: \mu_{1} = \mu{2} = \mu_{3}$
- $H_{1}$: Las medias poblacionales de las muestras no son iguales

donde $\mu_{1}$ es el precio de medicamentos de tipo 'Medicamento Etica', $\mu{2}$ el precio de medicamentos de tipo 'Medicamento Generico' y $\mu_{3}$ el precio de medicamentos de tipo 'Medicamento con Denominacion Generica'.

#### Resolución

Para realizar este contraste, se calculará el análisis de la varianza mediante ANOVA.

```{r}
kruskal.test(PVP ~ as.factor(Tipo), data = datos)
```

Como observamos, la variable tipo es relevante a la hora de definir el precio del medicamento, por lo que rechazamos la hipótesis nula. Por lo tanto, podemos confirmar que existen diferencias en el precio según el tipo de medicamento.

## 6.2 Precio vs. Laboratorio

En este apartado buscamos ver si existen diferencias en el precio del medicamento dependiendo de que laboratorio ha generado el medicamento. 

#### Hipótesis nula y alternativa

- $H_{0}: \mu_{1} = \mu{2} = ... = \mu{n}$
- $H_{1}$: Las medias poblacionales de las muestras no son iguales

donde μ representa los precios de los distintos laboratorios.

#### Resolución

Aplicaremos un test no paramétrico, el test Kruskal-Wallis, equivalente a ANOVA.

```{r}
kruskal.test(PVP ~ as.factor(Laboratorio), data = datos)
```

Como el p-value obtenido es menos de 0.001, sabemos que las modificaciones que la variable Laboratorio obtiene son relevantes en cuanto al PVP. Por lo tanto, tal y como en el ejercicio anterior, rechazamos la hipótesis nula. Esto quiere decir que el precio del medicamento es influido por el laboratorio que lo genera.

Nosotros tenemos la idea de que la variable Laboratorio no es realmente la que influya en el precio sinó en el hecho de que tenemos medicamentos para tratamientos muy específicos asociados a laboratorios muy específicos y por lo tanto este hecho hace que haya confusión entre variables. 

## 6.3 Precio vs. Enfermedad

En este último contraste la idea que queremos plasmar es el hecho de si existe o no existe diferencias en el precio del medicamento según la enfermedad a la que se quiere combatir. Este contraste es un poco obvio pero nos sirve para validar si realmente nuestra base de datos tiene los supósitos de que medicamentos más caros serviran para atacar a enfermedades diferentes. 

#### Hipótesis nula y alternativa

- $H_{0}: \mu_{1} = \mu{2} = ... = \mu{n}$
- $H_{1}$: Las medias poblacionales de las muestras no son iguales

donde μ representa los precios de las distintas enfermedades relacionadas.

#### Resolución

Aplicamos de nuevo el test Kruskal-Wallis:

```{r}
kruskal.test(PVP ~ as.factor(Enfermedad), data = datos)
```

Como en el caso anterior, al obtener un p-value menor que 0.001, sabemos que las modificaciones que la variable Enfermedad obtiene son relevantes en cuanto al PVP. Rechazamos la hipótesis nula, esto quiere decir que la enfermedad es influyente respecto al precio.

Esta conclusión ya la teniamos presente ya que tenemos medicamentos que sirven para paliar enfermedades crónicas graves y luego también tenemos medicamentos que sirven para tratar enfermedades como dolor de cabeza. 

# 7. Predicciones

En este apartado lo que se va a intentar es utilizar diferentes herramientas de predicción o de agrupación automática para sacar conclusiones de patrones subyacentes en nuestros datos que hayamos sido incapaces de detectar mediante las descriptiva univariante y bivariante. Para ello hemos escogido un método de predicción lineal y posteriormente dos métodos no supervisados: Clusterización jerárquica (para agrupar medicamentos con las mismas características) y Análisis de Corrrespondéncias Múltiples (ACM) (para detectar patrones de similitud de casos a través de correlaciones de variables). 

Para ello en cada apartado se explicará el uso de cada método de predicción. 

## 7.1 Regressión lineal

A continuación vamos a modelar una regresión lineal para poder identificar cuales són aquellas variables importantes que nos ayudan a predecir el valor del precio. 

Para ello estamos suponiendo que nuestros datos vienen distribuidos por una distribución normal que ya hemos demostrado anteriormente que no es correcto.

Para generar el modelo predictivo, lo que vamos a hacer será aplicar el modelo complejo (con todas las variables y sus interacciónes) y seguidamente le aplicaremos la función `stepAIC`. Esta función permite ir eliminando y incluyendo variables en el modelo según el error de AIC que aporte al generar el modelo. Cómo los modelos són generamos por el mismo tipo de variables, se pueden comparar con el criterio de Akaike (AIC). Se considerará que es el mejor modelo aquél que tenga inferior el AIC. 

De antemano ya confirmamos que nuestro modelo no será muy bueno devido a que estamos intentando predecir el precio del medicamento sólo con variables categóricas. 

```{r}
# A continuación generamos nuestro modelo lineal. Para ello primero de todo vamos a hacer los pasos de stepwise para que se escojan automaticamente aquellas variables que sean importantes para el modelo. 
library(MASS)

cols <- c("tram_PVP", "Codigo_Agrupacion")
dataRegresion <- datos_sel[, which(!colnames(datos_sel) %in% cols)]

full.model <- lm(PVP ~ ., data = dataRegresion)
step.model <- stepAIC(full.model, direction = "both", trace = FALSE)

summary(step.model)

```

Vemos que tenemos un $R^2$ ajustado entorno al 0.6078 por lo tanto sabemos que no es un buen modelo predictivo pero tampoco lo és malo ya que está por encima del 0.5. También decir que consideramos que sea un buen modelo predictivo aquél modelo que su $R^2$ esté por entorno al 0.7 o por encima. 

Mirando las variables que han entrado en el modelo podemos comprobar que sólo han sido 2: `Principio_Activo` y `Baja`. 

Esto nos está diciendo que sabiendo si el medicamento está de baja en la base de datos y cuál es su principio activo por el que está compuesto ya podemos obtener el precio del medicamento. 

También vemos que nuestro modelo no tiene constante y por lo tanto será directamente el precio igual al peso de la categoria correspondiente de nuestros datos. 

## 7.2 Agrupación de medicamentos (clúster)

A continuación vamos a clasificar nuestros datos a través de un modelo de agrupación jerárquico como són los clústers. Los métodos jerárquicos aplican una descomposición jerárquica del conjunto de datos origen. A partir del conjunto de datos por separado, va agrupando los grupos similares hasta formar un único grupo. Lo bueno de este tipo de modelos es que se puede visualizar el árbol que se genera al formar los grupos y decidir el número de clases en las que se quiere partir los datos.

```{r, eval = FALSE}
# install.packages("magrittr")
# install.packages("dendextend")
# install.packages("dendextendRcpp")

library(dendextend)
library(magrittr)
library(cluster)

datos_cluster <- datos_sel[, which(!colnames(datos_sel) %in% c('PVP', 'Codigo_Agrupacion', 'Codigo_Laboratorio', 'Principio_Activo'))]

for(i in colnames(datos_cluster)){
  datos_cluster[, i] <- as.factor(datos_cluster[, i])
}

k = 4

dend <- datos_cluster %>% daisy(metric = "gower", stand=TRUE) %>% hclust(method="ward.D") %>% as.dendrogram

labels(dend) <- rep(NA,nrow(datos_cluster))

dend %>% color_branches(k=k ,col = rainbow(k)) %>% plot(horiz=FALSE, main = "Método de Ward: 3 clusters",
                                              xlab='Participantes',ylab='Distancias') 

legend('topright',legend=c('Cluster 1','Cluster 2','Cluster 3', 'Cluster 4'), lty=rep(1,3), col=rainbow(k),cex=0.7)
abline(h = 33, lwd = 2, lty = 2, col = "blue")

c1 <- cutree(dend, k)

datos_cluster$CLUST4 <- as.factor(c1)

# Como hemos visto en el gráfico tenemos outliers en diferentes grupos que no se clasifican correctamente, por ello vamos a incorporar esos valores en su cluster correspondiente
```

Como podemos ver en nuestro clúster, utilizamos la distancia de Gower ya que es aquella distancia que nos permite trabajar con variables de tipo categóricas. También se le aplica el método de Ward. El método de Ward es aquel que nos permite asegurar mínima variancia entre los grupos que se van formando. 

En el gráfico podemos ver como podriamos formar 4 clases. El propósito era poder encontrar patrones de semejanza entre conjuntos de medicamentos y bajo una decisión experta, hemos decidido avanzar con el clúster de 4 clases. 

A continuación se muestra una descriptiva de las clases para las variables que hemos escogido para poder hacer un perfil de lo que encontramos en cada clase obtenida. Como todos las variables que hemos utilizado para generar el clúster han sido categóricas vamos a realizar unos gráficos agrupados y posteriormente un contraste de chi2 para ver realmente si existe o no independencia entre las clases de clúster y las variables explicativas. 

Así utilizando este procedimiento podemos ver de que variables definen nuestros grupos realizados por el clúster. 

```{r}
cols <- colnames(datos_cluster)

for (col in cols){
  tabla <- prop.table(ftable(formula(paste0("CLUST4 ~ ", col)), datos_cluster), 2)
  lbls <- unique(datos[, col])
  dimnames(tabla) <- list(lbls, paste0("Clust", levels(datos_cluster$CLUST4)))
  mosaicplot(tabla,col=rainbow(4),main = paste0(col, " según cluster"))
  chisq.test(ftable(formula(paste0("CLUST4 ~ ", col)), datos_cluster))-> test
  p <- if(test$p.value>.001) {paste("p =",round(test$p.value,3))} else {"p < 0.001"}
  mtext(side=1,bquote(chi^2== .(round(test$statistic,2)) ~~ .(p)),padj=0.5)
}


```

Como vemos en el gráfico y en los contrastes, no tenemos ninguna categórias de ninguna variable que nos indique independencia en el clúster por lo tanto las clases no se definen en concreto por ninguna categoria. 

Por lo tanto lo que estmaos viendo es que detrás de nuestros datos encontramos un patron de clasificador subyacente que no podemos detectar a simple vista ni describir por ninguna de las variables usadas en el modelo. 

Esta conclusión también la podemos afirmar viendo el contraste de $\chi^2$ de cada uno de los gráficos que nos expresa de que no existen evidéncias claras para poder decir que hay independencia entre los clústers y las castegorias de una variable. 


## 7.3 Detección de patrones en las características (ACM)

Para acabar el tema de los modelos supervisados y no supervisados, hemos decidido generar un modelo de análisis de correspondencias múltiples (ACM). El análisis de correspondéncias múltiples (ACM) se trata de un método destinado a variables nominales que se utiliza para detectar y representar patrones escondidos en los datos. 

La forma de proceder es la siguiente, se representa los datos como puntos en un espacio euclídeo de dimensión 2. La forma de proyección es la misma que se utilizaria para la creación de los ACP (Análisis de componentes princiales) peró con variables que tienen más de dos niveles en sus categorias. 

Por lo tanto esta forma de detectar patrones es muy eficaz para nuestro conjunto de datos ya que, como se han explicado en apartados anteriores, nuestros datos estan compuestos por variables categóricas y una variable numérica. 

```{r}
# install.packages("ade4", dep = TRUE)
library(ade4)

# Creamos los datos para el ACM descartando la variable numérica
datosACM <- datos_sel[, which(!colnames(datos_sel) %in% c('PVP', 'Codigo_Laboratorio', 'Codigo_Agrupacion', 'Principio_Activo'))]

for(i in colnames(datosACM)){
  datosACM[, i] <- as.factor(datosACM[, i])
}

# Generamos el modelo ACM
acm <- dudi.acm(datosACM, scannf = FALSE, nf = 10)

# Mostramos los gráficos
print(summary(acm))
```

Hemos tenido que descartar algunas de las variables debido al gran volumen de categorias que tenian esas variables. De aquí deberemos extraer una de las posibles mejoras del trabajo que seria el hecho de agrupar mejor categorias ya que con tantas categorias, el ACM no es comprensible visualmente. 

Teniendo en cuenta las limitaciones anteriores, vamos a interpretar tanto la salida de la inercia que contienen nuestros datos como la visualización de las categorias. 

Por lo que hace la salida del resumen de la inercia, vemos como sólo podemos explicar el 16.278% de la variancia de las categorias con tan solo dos dimensiones. Esto lo que nos permite identificar es que necesitamos más dimensiones para poder explicar la variancia de los datos. Por lo tanto aunque explicaremos dos dimensiones no seremos capaces de identificar todos los posibles patrones subyacentes que hay entre categorias de variables. 

```{r}
# Generamos el gráfico de aportación de cada variable al primer y al segundo eje
par(mfrow = c(2, 2))
for (i in 1:2) barplot(acm$cr[, i], names.arg = row.names(acm$cr), 
  las = 2, main = paste("Eje", i))
par(mfrow = c(1, 1))
```

Como podemos ver, la variable `Enfermedad`, es una variable que explica muy bien tanto la primera como la segunda dimension. Este hecho nos puede hacer pensar que es lo que nos pueden explicar los ejes. Viendo el histograma primero, vemos que para la primera dimension las variables `Enfermedad` y `Aportacion_Beneficiario` son las mas representativas. Por lo tanto podriamos definir que el primer eje habla de las características del paciente. El segundo eje lo podriamos definir como las características del uso del medicamento ya que vemos muy expresadas las variables `Estado`, `Baja`, `Tratamiento_Larga_Duracion` y `Enfermedad`. 

Por lo tanto en resumen, podemos definir los ejes de la siguiente manera:
  - 1r eje: Características del paciente
  - 2n eje: Características del uso del medicamento

A continuación realizamos el gráfico. 

```{r}
# Generamos el gráfico de proyección de ACM
s.label(acm$co, clabel = 0.6, boxes = FALSE)
```

Aunque el gráfico no es del todo aclarador debido a que hay muchas categorias que solapan algunas etiquetas, podemos distingir que:

- Medicamentos que sirven para el tratamiento de la diabetis o anticoagulantes se sirven o se presentan en formato injectable. 
- Los dolores musculares no se agrupan de ninguna forma en concreta. 

-Podemos ver como los dolores neuropáticos se suministran con inhaladores. Nos extraña esta conclusión pero también cabe decir que estas categorias se encuentran cerca de los ejes x e y y por lo tanto significa que sus proyecciones no se representan de forma correcta. 

- Otra de las conclusiones que podemos sacar es que aquellos medicamentos que son de tipo ético o tratamiento ético són aquellos que su coste está entre los 30 hasta los 322€. Por lo tanto vemos que la medicina ética es costosa y por eso són los hospitales que hacen uso de ellos. 

- También podemos concluir que aquellas enfermedades de Hipertiroidismo tienen descuento en el precio del medicamento ya que se les catalóga como aportación ESPECIAL. Por lo tanto en los medicamentos relacionados con esas enfermedades tendrán un descuento más grande que otras enfermedades. 

Si aumentaramos la zona central podriamos llegar a encontrar mas relaciones entre categorias. Cabe decir que no vamos a gastar recursos en hacerlo porque tal y como hemos explicado anteriormente, sus proyecciones estan muy cerca del centro de los ejes y eso nos está diciendo que no se representan correctamente. 

Mediante el paquete `explor` de R permite a través de un shiny realizar el estudio ACM al completo. 

# 8. Conclusiones y futuras lineas de trabajo

Como conclusiones del trabajo podemos decir que nos hemos dado cuenta de la importáncia del pre-processing de los datos ya que mucha de la información no se encuentra de forma tabular sinó que se han de seguir trabajando para obtenerlas tal y como las necesitamos. 

También nos hemos dado cuenta de la importáncia de la transformación de los datos para obtener conclusiones a partir de los modelos. 

También decir que hemos ido concluyendo cada apartado según ivamos resolviendo los problemas y por lo tanto se ha ido concluyendo cada apartado en el mismo. 

A continuación se detallarán futuras lineas de trabajo que se podrian realizar a partir de los datos que se han analizado. Decir que estas futuras lineas de trabajo se deberian de adecuar a las necesidades de cada usuario de los datos por lo tanto puede ser que para según quien use estos datos no le hagan falta algunas de las propuestas que se utilizarán a continuación:

  - Generar, a partir del modelo predictivo del árbol de decisión, un app que le permita a cualquier usuario médico encontrar rápidamente el medicamento mas adecuado para cada paciente.
  - Realizaría más text mining a la variable Medicamento e Presentación para extraer más características de los productos que no se han extraido ni se han podido extraer.
  - Dedicarle más tiempo al tratamiento de variables de texto y hacer agrupaciones de categorias de variables.
  - Descargar información más completa de medicamentos (que sean aleatoria para generar supuestos)
  - Para acabar decir que se podria desarrollar mejor el apartado de ACM en el cuál podriamos detectar muchos mas patrones de los datos por tal de obtener conclusiones más acertadas. 

# 9. Código

El código del proyecto se encuentra en el repositorio libre de Github. [Enlace](https://github.com/lmgeraldino/vademecum-consolidation).

  
